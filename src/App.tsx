import React, { useState, useEffect, useRef } from "react";
import { createDeepgramService, DeepgramService, TranscriptEvent } from "./services/deepgram";
import { createClaudeService, ClaudeAnalysisService, AnalysisContext, InsightResponse, ClaudeServiceConfig } from "./services/claude";
import { configService } from "./services/config";
import { LegacyInsight } from "./types/events";
import { StartScreen, RecordingScreen, WaveLoader } from "./components";
import { useAudioAnalyser } from "./hooks/useAudioAnalyser";
import { PostEditorConfig, CorrectionContext } from "./services/post-editor";
import { endCurrentSession } from "./services/transcript-logger";

// –¢–∏–ø—ã –¥–ª—è IPC
declare global {
  interface Window {
    require: any;
  }
}

export function App() {
  // –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –æ–∫–Ω–∞ –∏–∑ URL –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
  const urlParams = new URLSearchParams(window.location.search);
  const windowType = urlParams.get('window') || 'control'; // 'control' –∏–ª–∏ 'data'
  


  const [isVisible, setIsVisible] = useState(true);
  const [clickThrough, setClickThrough] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [hasPermission, setHasPermission] = useState<boolean | null>(null);
  const [transcript, setTranscript] = useState<string>('');
  const [partialTranscript, setPartialTranscript] = useState<string>('');
  const [insights, setInsights] = useState<Array<LegacyInsight>>([]);
  const [fullTranscript, setFullTranscript] = useState<string>(''); // –ù–∞–∫–∞–ø–ª–∏–≤–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
  const [showTranscript, setShowTranscript] = useState(false);
  const [correctedSegments, setCorrectedSegments] = useState<Map<string, string>>(new Map()); // segment_id -> corrected text
  const [lastCorrectionTime, setLastCorrectionTime] = useState<number>(0);
  // Removed background blur functionality
  
  // mediaRecorderRef —É–¥–∞–ª–µ–Ω - –±–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–µ–Ω
  const streamRef = useRef<MediaStream | null>(null);
  const deepgramRef = useRef<DeepgramService | null>(null);
  const claudeRef = useRef<ClaudeAnalysisService | null>(null);
  const analysisContextRef = useRef<AnalysisContext | null>(null);
  // const ragServiceRef = useRef<RAGService | null>(null);
  const cleanupRef = useRef<(() => void) | null>(null);

  // Audio analysis hook
  const { audioLevel, initAudioAnalyser, stopAudioAnalyser } = useAudioAnalyser();

  // –§—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
  const addSampleDocuments = async () => {
    // if (!ragServiceRef.current) {
      console.log('üìù RAG service disabled, skipping sample documents');
      return;
    // }

    // RAG system temporarily disabled
    console.log('‚úÖ RAG system disabled - skipping document loading');
  };

  // Cleanup audio analyser when recording stops
  useEffect(() => {
    if (!isRecording) {
      stopAudioAnalyser();
    }
  }, [isRecording, stopAudioAnalyser]);

  useEffect(() => {
    console.log('üöÄ App component mounted');
  }, []);

  useEffect(() => {
    const handleKeydown = (e: KeyboardEvent) => {
      if ((e.ctrlKey || e.metaKey) && e.key === "\\") {
        setIsVisible(!isVisible);
      }
      if ((e.ctrlKey || e.metaKey) && e.shiftKey && e.key === "T") {
        setClickThrough(!clickThrough);
      }
    };
    window.addEventListener("keydown", handleKeydown);
    return () => window.removeEventListener("keydown", handleKeydown);
  }, [isVisible, clickThrough]);

  useEffect(() => {
    console.log('üé§ Checking microphone permission...');
    checkMicPermission().catch(error => {
      console.warn('‚ö†Ô∏è Mic permission check failed:', error);
    });
  }, []);

  const checkMicPermission = async () => {
    try {
      console.log('Checking microphone permission...');
      const audioConstraints = configService.getAudioConstraints();
      const stream = await navigator.mediaDevices.getUserMedia(audioConstraints);
      console.log('Microphone permission granted!');
      setHasPermission(true);
      stream.getTracks().forEach(track => track.stop());
    } catch (error) {
      console.error('Microphone permission denied:', error);
      setHasPermission(false);
    }
  };

  const connectToDeepgram = async (): Promise<() => void> => {
    console.log('üîó [DEEPGRAM] Starting Deepgram connection...');
    
    // –õ–æ–≥–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤ dev —Ä–µ–∂–∏–º–µ
    if (configService.isDevelopment) {
      console.log('üîß [DEEPGRAM] Development mode - logging config...');
      configService.logConfig();
    }
    
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Deepgram
    if (!configService.isDeepgramConfigured()) {
      throw new Error('‚ùå [DEEPGRAM] API key not configured! Please add DEEPGRAM_API_KEY to .env file');
    }
    
    console.log('‚úÖ [DEEPGRAM] API key configured, proceeding with real connection...');

    // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º RAG —Å–µ—Ä–≤–∏—Å (–≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω)
    console.log('üìù RAG service temporarily disabled for debugging');

    // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Claude —Å–µ—Ä–≤–∏—Å –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
    if (configService.isClaudeConfigured()) {
      try {
        const claudeConfig = configService.getClaudeConfig();
        claudeRef.current = createClaudeService(claudeConfig); // RAG disabled
        analysisContextRef.current = new AnalysisContext();
        console.log('‚úÖ Claude service initialized with RAG support:', {
          model: claudeConfig.model,
          maxTokens: claudeConfig.maxTokens,
          temperature: claudeConfig.temperature,
          ragEnabled: false // temporarily disabled
        });
      } catch (error) {
        console.warn('‚ö†Ô∏è Claude service failed to initialize:', error);
      }
    } else {
      console.log('‚ö†Ô∏è Claude API key not configured, insights will be limited...');
    }

    try {
      console.log('üì° [DEEPGRAM] Connecting to real Deepgram...');
      
      const deepgramConfig = configService.getDeepgramConfig();
      console.log('üîß [DEEPGRAM] Using config:', {
        model: deepgramConfig.model,
        language: deepgramConfig.language,
        interim_results: deepgramConfig.interim_results,
        endpointing: deepgramConfig.endpointing
      });
      
      // –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ—Å—Ç—Ä–µ–¥–∞–∫—Ç–æ—Ä–∞
      let postEditorConfig: PostEditorConfig | undefined;
      let correctionContext: CorrectionContext | undefined;
      
      if (configService.isPostEditorConfigured()) {
        postEditorConfig = configService.getPostEditorConfig();
        correctionContext = {
          jobTerms: [], // –ë—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–æ –ø–æ–∑–∂–µ
          synonymDictionary: {} // –ë—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –≤ post-editor.ts
        };
        console.log('üîß Post-editor enabled:', {
          model: postEditorConfig.model,
          timeout: postEditorConfig.timeoutMs
        });
      } else {
        console.log('‚ö†Ô∏è Post-editor not configured, skipping...');
      }
      
      const deepgram = createDeepgramService(
        deepgramConfig.apiKey,
        async (event: TranscriptEvent) => {
          console.log('üìù [TRANSCRIPT] Received Deepgram event:', {
            type: event.type,
            text: event.text?.substring(0, 50) + '...',
            confidence: event.confidence,
            timestamp: new Date().toLocaleTimeString()
          });
          
          if (event.type === 'partial') {
            const newPartial = event.text;
            setPartialTranscript(newPartial);
            console.log('üîÑ [PARTIAL] Deepgram partial:', newPartial);
            
            // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ –æ–∫–Ω–æ –¥–∞–Ω–Ω—ã—Ö
            if (window.require) {
              const { ipcRenderer } = window.require('electron');
              ipcRenderer.invoke('send-transcript', transcript, newPartial);
            }
          } else if (event.type === 'final') {
            // –û–±–Ω–æ–≤–ª—è–µ–º –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º—ã–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç
            const newTranscript = (transcript + ' ' + event.text).trim();
            setTranscript(newTranscript);
            setPartialTranscript('');
            
            // –ù–∞–∫–∞–ø–ª–∏–≤–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç
            setFullTranscript(prev => (prev + ' ' + event.text).trim());
            
            console.log('‚úÖ [FINAL] Deepgram final result:', {
              text: event.text,
              confidence: event.confidence,
              length: event.text.length,
              wordCount: event.text.split(' ').length
            });
            
            // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ –æ–∫–Ω–æ –¥–∞–Ω–Ω—ã—Ö
            if (window.require) {
              const { ipcRenderer } = window.require('electron');
              ipcRenderer.invoke('send-transcript', newTranscript, '');
            }
            
            // –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å –ø–æ–º–æ—â—å—é Claude –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
            if (event.text.length > 10) {
              await analyzeWithClaude(event.text);
            }
          }
        },
        (error: string) => {
          console.error('‚ùå [DEEPGRAM] Error:', error);
          setInsights(prev => [...prev.slice(-2), {
            id: Date.now().toString(),
            text: `Deepgram API error: ${error}`,
            type: 'risk'
          }]);
        },
        deepgramConfig,
        postEditorConfig,
        correctionContext
      );

      deepgramRef.current = deepgram;
      await deepgram.connect();
      
      return () => {
        deepgram.disconnect();
        deepgramRef.current = null;
        claudeRef.current = null;
        analysisContextRef.current = null;
      };
      
    } catch (error) {
      console.error('‚ùå [DEEPGRAM] Failed to connect to Deepgram:', error);
      throw error; // –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –æ—à–∏–±–∫—É –¥–∞–ª—å—à–µ - –ù–ò–ö–ê–ö–ò–• –ú–û–ö–û–í!
    }
  };

  // –ê–Ω–∞–ª–∏–∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ —Å –ø–æ–º–æ—â—å—é Claude AI
  const analyzeWithClaude = async (newText: string): Promise<void> => {
    if (!claudeRef.current || !analysisContextRef.current) {
      console.log('‚ö†Ô∏è Claude not available, skipping analysis...');
      return;
    }

    try {
      // –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π —Ç–µ–∫—Å—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç
      analysisContextRef.current.addTranscript(newText);
      const context = analysisContextRef.current.getContext();

      // –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
      const analysisRequest = {
        transcript: newText,
        contextWindow: context.contextWindow,
        entities: context.entities,
        topicHistory: context.topicHistory
      };

      console.log('ü§ñ Analyzing with Claude...', {
        text_length: newText.length,
        context_words: context.contextWindow.length,
        entities: context.entities.length
      });

      // –ü–æ–ª—É—á–∞–µ–º –∞–Ω–∞–ª–∏–∑ –æ—Ç Claude
      const analysis: InsightResponse = await claudeRef.current.analyzeTranscript(analysisRequest);

      // –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ø–∏–∫ –≤ –∏—Å—Ç–æ—Ä–∏—é
      analysisContextRef.current.addTopic(analysis.topic);

      // –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç legacy insight –¥–ª—è UI
      const legacyInsight: LegacyInsight = {
        id: Date.now().toString(),
        text: analysis.note,
        type: analysis.type
      };

      // –û–±–Ω–æ–≤–ª—è–µ–º UI (–ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 insights)
      const newInsights = [...insights.slice(-2), legacyInsight];
      setInsights(newInsights);

      // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ –æ–∫–Ω–æ –¥–∞–Ω–Ω—ã—Ö
      if (window.require) {
        const { ipcRenderer } = window.require('electron');
        ipcRenderer.invoke('send-insights', newInsights);
      }

      console.log('‚úÖ Claude analysis complete:', {
        topic: analysis.topic,
        depth_score: analysis.depth_score,
        type: analysis.type,
        confidence: analysis.confidence
      });

    } catch (error) {
      console.error('‚ùå Claude analysis failed:', error);
      // –ù–ò–ö–ê–ö–ò–• –ú–û–ö–û–í - –ø—Ä–æ—Å—Ç–æ –ª–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É
    }
  };

  // –ú–û–ö –§–£–ù–ö–¶–ò–ò –£–î–ê–õ–ï–ù–´ - –ù–ò–ö–ê–ö–ò–• –§–ï–ô–ö–û–í!

  const startRecording = async () => {
    try {
      console.log('üé¨ [STEP 1] Starting recording...');
      
      // –°–ù–ê–ß–ê–õ–ê –º–µ–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ UI –¥–ª—è –º–≥–Ω–æ–≤–µ–Ω–Ω–æ–π —Ä–µ–∞–∫—Ü–∏–∏
      setIsRecording(true);
      console.log('‚úÖ [STEP 2] UI state updated to recording');
      
      // –°–æ–∑–¥–∞–µ–º –æ–∫–Ω–æ —Å –¥–∞–Ω–Ω—ã–º–∏ –ø—Ä–∏ –Ω–∞—á–∞–ª–µ –∑–∞–ø–∏—Å–∏
      if (window.require) {
        const { ipcRenderer } = window.require('electron');
        ipcRenderer.invoke('create-data-window'); // –£–±–∏—Ä–∞–µ–º await –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        console.log('üì± [STEP 3] Data window creation requested');
      }
      
      // –û—Å—Ç–∞–ª—å–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤—ã–ø–æ–ª–Ω—è–µ–º –≤ —Ñ–æ–Ω–µ
      const audioConstraints = configService.getAudioConstraints();
      console.log('üé§ [STEP 4] Getting microphone access with constraints:', audioConstraints);
      
      const stream = await navigator.mediaDevices.getUserMedia(audioConstraints);
      console.log('‚úÖ [STEP 5] Got media stream:', {
        id: stream.id,
        active: stream.active,
        tracks: stream.getTracks().length
      });
      streamRef.current = stream;
      
      // Initialize audio analyser for SiriWave
      console.log('üéµ [STEP 6] Initializing audio analyser...');
      initAudioAnalyser(stream);
      
      // Connect to Deepgram (REAL ONLY - NO MOCKS!)
      console.log('üì° [STEP 7] Connecting to Deepgram...');
      try {
        cleanupRef.current = await connectToDeepgram();
        console.log('‚úÖ [STEP 8] Deepgram connection established');
      } catch (error) {
        console.error('‚ùå [DEEPGRAM] Connection failed:', error);
        setIsRecording(false);
        setHasPermission(false);
        const errorMessage = error instanceof Error ? error.message : String(error);
        throw new Error(`Deepgram connection failed: ${errorMessage}`);
      }
      
      // –ò—Å–ø–æ–ª—å–∑—É–µ–º AudioWorkletNode –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è PCM –¥–∞–Ω–Ω—ã—Ö
      const audioConfig = configService.audio;
      console.log('üîß [STEP 9] Creating AudioContext with config:', audioConfig);
      const audioContext = new AudioContext({ sampleRate: audioConfig.sampleRate });
      
      try {
        // –ó–∞–≥—Ä—É–∂–∞–µ–º Audio Worklet
        console.log('‚öôÔ∏è [STEP 10] Loading AudioWorklet module...');
        await audioContext.audioWorklet.addModule('/audioWorklet.js');
        console.log('‚úÖ [STEP 11] AudioWorklet module loaded');
        
        const source = audioContext.createMediaStreamSource(stream);
        const workletNode = new AudioWorkletNode(audioContext, 'pcm-processor');
        console.log('üîó [STEP 12] AudioWorklet nodes created');
        
        // –°—á–µ—Ç—á–∏–∫ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø–æ—Ç–æ–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        let chunkCount = 0;
        let totalBytes = 0;
        
        // –°–ª—É—à–∞–µ–º PCM –¥–∞–Ω–Ω—ã–µ –æ—Ç worklet
        workletNode.port.onmessage = (event) => {
                  if (event.data.type === 'pcm-data' && deepgramRef.current) {
          chunkCount++;
          totalBytes += event.data.data.byteLength;
          
          // –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π 10-–π —á–∞–Ω–∫ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
          if (chunkCount % 10 === 0) {
            console.log(`üìä [AUDIO FLOW] Chunk #${chunkCount}: ${event.data.data.byteLength} bytes (total: ${(totalBytes/1024).toFixed(1)}KB)`);
          }
          
          deepgramRef.current.sendAudio(event.data.data);
        }
        };
        
        source.connect(workletNode);
        workletNode.connect(audioContext.destination);
        
        // –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫–∏ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏
        streamRef.current = stream;
        cleanupRef.current = () => {
          workletNode.disconnect();
          source.disconnect();
          audioContext.close();
          if (deepgramRef.current) {
            deepgramRef.current.disconnect();
            deepgramRef.current = null;
          }
          claudeRef.current = null;
          analysisContextRef.current = null;
        };
        
      } catch (error) {
        console.error('‚ùå AudioWorklet –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è:', error);
        throw new Error(`AudioWorklet not supported: ${error instanceof Error ? error.message : String(error)}`);
      }
      console.log('Recording started with Deepgram AudioWorklet!');
    } catch (error) {
      console.error('Error starting recording:', error);
      setHasPermission(false);
    }
  };

  // FALLBACK –§–£–ù–ö–¶–ò–ò –£–î–ê–õ–ï–ù–´ - –ù–ò–ö–ê–ö–ò–• –û–ë–•–û–î–ù–´–• –ü–£–¢–ï–ô!

  const stopRecording = () => {
    // –ó–∞–∫—Ä—ã–≤–∞–µ–º –æ–∫–Ω–æ —Å –¥–∞–Ω–Ω—ã–º–∏ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ –∑–∞–ø–∏—Å–∏
    if (window.require) {
      const { ipcRenderer } = window.require('electron');
      ipcRenderer.invoke('close-data-window');
    }
    
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    if (cleanupRef.current) {
      cleanupRef.current();
      cleanupRef.current = null;
    }
    
    // –ó–∞–≤–µ—Ä—à–∞–µ–º —Å–µ—Å—Å–∏—é –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    try {
      endCurrentSession();
      console.log('üìä Transcript session ended and saved to file');
    } catch (error) {
      console.warn('‚ö†Ô∏è Failed to end transcript session:', error);
    }
    
    // –û—á–∏—â–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–µ—Ä–≤–∏—Å—ã
            deepgramRef.current = null;
    claudeRef.current = null;
    if (analysisContextRef.current) {
      analysisContextRef.current.reset();
      analysisContextRef.current = null;
    }
    
    setIsRecording(false);
    setPartialTranscript('');
    // –û—Å—Ç–∞–≤–ª—è–µ–º transcript –∏ insights –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –ø–æ—Å–ª–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
    console.log('Recording stopped, session data preserved');
  };

  if (!isVisible) {
    return <div className="w-full h-full bg-transparent" />;
  }

  // –†–µ–Ω–¥–µ—Ä —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –æ–∫–æ–Ω
  if (windowType === 'data') {
    // –û–∫–Ω–æ —Å –¥–∞–Ω–Ω—ã–º–∏ - —Å–ª—É—à–∞–µ–º IPC —Å–æ–±—ã—Ç–∏—è
    useEffect(() => {
      if (window.require) {
        const { ipcRenderer } = window.require('electron');
        
        // –°–ª—É—à–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞
        const handleTranscriptUpdate = (event: any, data: any) => {
          setTranscript(data.transcript || '');
          setPartialTranscript(data.partialTranscript || '');
        };

        // –°–ª—É—à–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏–Ω—Å–∞–π—Ç–æ–≤
        const handleInsightsUpdate = (event: any, newInsights: any) => {
          setInsights(newInsights || []);
        };

        ipcRenderer.on('transcript-update', handleTranscriptUpdate);
        ipcRenderer.on('insights-update', handleInsightsUpdate);

        return () => {
          ipcRenderer.removeListener('transcript-update', handleTranscriptUpdate);
          ipcRenderer.removeListener('insights-update', handleInsightsUpdate);
        };
      }
    }, []);

    // –û–∫–Ω–æ —Å –¥–∞–Ω–Ω—ã–º–∏ (—Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è + –∏–Ω—Å–∞–π—Ç—ã)
    return (
      <div className="w-full h-full bg-white/80 backdrop-blur-md p-4">
        <div className="content-panel">
          <div className="content-section">
            <div className="content-section__title">Transcript</div>
            <div className="content-section__content">
              {transcript || <span className="transcript-text--placeholder">Waiting for speech...</span>}
              {partialTranscript && (
                <span className="transcript-text--partial"> {partialTranscript}</span>
              )}
            </div>
          </div>
          
          <div className="content-section">
            <div className="content-section__title">AI Insights</div>
            <div className="content-section__content">
              {insights.length > 0 ? (
                insights.map((insight) => (
                  <div key={insight.id} className="insight-item">
                    {insight.text}
                  </div>
                ))
              ) : (
                <div className="insight-placeholder">Analyzing speech patterns...</div>
              )}
            </div>
          </div>
        </div>
      </div>
    );
  }

  // –ï–¥–∏–Ω–∞—è –ø–∞–Ω–µ–ª—å —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å –∞–Ω–∏–º–∞—Ü–∏–µ–π –º–µ–∂–¥—É —Å–æ—Å—Ç–æ—è–Ω–∏—è–º–∏
  return (
    <div className={`w-full h-full ${isVisible ? 'opacity-100' : 'opacity-0'} transition-opacity duration-300`} 
         style={{ pointerEvents: clickThrough ? 'none' : 'auto' }}>
      
      <div className="w-full h-full flex items-center justify-center">
        <div className={`control-panel ${isRecording ? 'control-panel--recording' : ''} transition-all duration-500 ease-in-out`}>
          <div className="control-panel__actions">
            {/* Start/Stop Button —Å –∞–Ω–∏–º–∞—Ü–∏–µ–π */}
            <div className="button-container">
              {!isRecording ? (
                <button
                  onClick={hasPermission ? startRecording : checkMicPermission}
                  className="start-button animate-fade-in"
                  style={{WebkitAppRegion: 'no-drag'} as any}
                >
                  <div className="start-button__icon">
                    <div className="start-button__icon-rect"></div>
                    <svg className="start-button__icon-svg" viewBox="0 0 8 10" fill="none">
                      <ellipse cx="4" cy="8" rx="4" ry="2" stroke="white" strokeWidth="1.4"/>
                    </svg>
                  </div>
                  <span>{hasPermission ? 'Start' : 'Allow Mic'}</span>
                </button>
              ) : (
                <button
                  onClick={stopRecording}
                  className="stop-button animate-fade-in"
                  style={{WebkitAppRegion: 'no-drag'} as any}
                >
                  <div className="stop-button__icon"></div>
                </button>
              )}
            </div>
            
            {/* Wave Loader —Å –∞–Ω–∏–º–∞—Ü–∏–µ–π –ø–æ—è–≤–ª–µ–Ω–∏—è/–∏—Å—á–µ–∑–Ω–æ–≤–µ–Ω–∏—è */}
            <div className={`wave-loader-wrapper transition-all duration-500 ease-in-out ${
              isRecording 
                ? 'opacity-100 translate-x-0 w-auto' 
                : 'opacity-0 -translate-x-4 w-0 overflow-hidden'
            }`}>
              <WaveLoader
                isActive={isRecording}
                audioLevel={audioLevel}
                partialTranscript={partialTranscript}
                width={20}
                height={15}
              />
            </div>
          </div>
          
          <div className="control-panel__separator"></div>
          
          <div 
            className="control-panel__drag-zone"
            style={{WebkitAppRegion: 'drag'} as any}
          >
            <div className="drag-dots">
              <div className="drag-dots__dot"></div>
              <div className="drag-dots__dot"></div>
              <div className="drag-dots__dot"></div>
              <div className="drag-dots__dot"></div>
            </div>
          </div>
        </div>
      </div>
      
      {/* Click-through –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä */}
      {clickThrough && (
        <div className="click-through-indicator">
          Click-through mode
        </div>
      )}
    </div>
  );
}
